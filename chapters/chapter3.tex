\chapter{基于服务端中心计算的联邦学习框架}

\section{问题描述}
\ensection{Problem Description}

所有的联邦学习框架的核心思想都是在不共享数据的前提下进行模型训练。
这就意味着在联邦学习框架下，模型的训练数据是分布在多个数据持有者的设备上的，而这些设备的计算能力和存储能力往往是有限的。
存在严重的算力不均衡的问题，这就导致了在联邦学习框架下，模型的训练效率往往是非常低的。
这就是联邦学习中的计算异构问题。为了解决这一问题，
我们提出了一种新的联邦学习框架（MedFed），该框架可以在不共享数据的前提下实现大规模模型的高效训练。

同时联邦学习还有一个重要的问题是非独立同分布数据的问题。由于未标注数据和标注数据，
不同的联邦学习参与者的数据分布往往是不同的，对模型性能有着不同程度的损害。
为了解决联邦学习中的数据异构问题，之前已经有了一些工作，
比如FedAvg\supercite{DBLP:journals/corr/McMahanMRA16}、
FedProx\supercite{DBLP:journals/corr/abs-1812-06127}、
FedOpt\supercite{DBLP:journals/corr/abs-1910-12093}、
FedNova\supercite{DBLP:journals/corr/abs-2003-00295}、
FedMA\supercite{DBLP:journals/corr/abs-2007-13518}、
FedPD\supercite{DBLP:journals/corr/abs-2007-13518}、
FedMGDA\supercite{DBLP:journals/corr/abs-2007-13518}等。

相关工作。联邦学习是一个快速发展的主题，我们只在这里描述与之密切相关的方法。关于联邦学习的综合研究已经出现在Kairouz等人（2019）；Li等人（2020）的文献中。
一般的联邦学习设置涉及两种类型的更新，即服务器端和设备端，每种更新都与最小化某种局部损失函数相关联，该损失函数本身可以在不同轮次动态更新。
在任何一轮中，有些方法试图完全优化，而其他方法则提出了近似优化。我们专注于解决四种联邦学习场景（大规模分发、异质性、不可靠链接和数据不平衡）的相关工作。
一种工作提出了基于本地SGD（Stich, 2019）的更新，其中每个参与设备执行单个本地SGD步骤。然后服务器平均接收到的模型。与本地SGD相比，我们的方法提出了最小化本地惩罚经验损失。
FedAvg（McMahan等人，2017）是本地SGD的一般化，提出了每轮更多的本地SGD步骤。尽管如此，FedAvg仍然不完全解决了设备端的优化问题。
确定何时停止最小化以获得良好的精度-通信折衷是基于调整时期数和学习率的（McMahan等人，2017；Li等人，2020b）。
尽管FedAvg在IID设置中具有较强的经验性能，但在非IID场景中性能下降（Zhao等人，2018）。
已经提出了几种修改FedAvg以处理非IID设置的方法。这些变体包括使用递减的学习率（Li等人，2020b）；动态修改设备经验损失（Li等人，2020a）；或修改服务器端更新（Hsu等人，2019；Reddi等人，2020）。
使用递减学习率或定制服务器端更新的方法仍然依赖于设备内的本地SGD更新。虽然这些工作确实承认了本地和全局稳定点的不兼容性，但其提出的修复方法基于近似最小化。
此外，为了在非IID情况下建立收敛性，这些工作还施加了额外的“有界非IID”条件。
FedProx（Li等人，2020a）与我们的方法相关。与我们一样，他们提出了一种动态正则化器，该正则化器基于服务器提供的模型进行修改。
然而，由此产生的正则化器并不能使全局和本地稳定点对齐，因此需要进行近似最小化，他们通过精心选择学习率和时期来实现这一点。此外，调整需要对统计异质性有一定的了解。
类似的，还有一些工作将更新与额外的设备变量相结合，这些变量也与模型一起传输（Karimireddy等人，2019；Shamir等人，2014）。
这些工作通过添加设备相关的正则化器来证明收敛性保证。
然而，它们会增加额外的通信成本，并且在深度神经网络方面没有进行广泛的实验。其中，SCAFFOLD（Karimireddy等人，2019）是一项与本文密切相关的工作，尽管它传输了额外的变量，但在第2节中给出了更详细的比较。
另一类分布式优化方法（Konecˇny`等人，2016；Makhdoumi＆Ozdaglar，2017；Shamir等人，2014；Yuan＆Ma，2020；Pathak＆Wainwright，2020；Liang等人，2019；Li等人，2020c；Condat等人，2020）可以在这种设置中考虑。
此外，还有一些工作将SGD类型方法的分析扩展到FL设置中（Gorbunov等人，2020；Khaled等人，2020b；Li＆Richta ́rik，2020）。
然而，这些算法是为全设备参与的情况提出的，未能满足FL的一个重要方面。FedSVRG（Konecˇny`等人，2016）和DANE（Shamir等人，2014）需要每
轮从所有设备获取梯度信息，因此不直接适用于部分FL设置。例如，FedDANE（Li等人，2019）是DANE的一个版本，可在部分参与中运行。
然而，FedDANE在部分参与的情况下实验性能比FedAvg差（Li等人，2019）。与这些工作类似，FedPD（Zhang等人，2020）方法是在不同参与方式的分布式优化中提出的。
FedPD每轮激活所有设备或不激活任何设备，这也不能满足FL的部分参与要求。
最后，另一类工作旨在通过压缩传输模型来减少通信成本（Dutta等人，2019；Mishchenko等人，2019；Alistarh等人，2017）。
通过降低传输的比特率来节省通信成本。这些想法与我们的工作是互补的，它们可以集成到我们提出的解决方案中。

与此同时，联邦学习还有一个问题是说，不同的客户端的通信带宽和延迟是不同的。这就导致了在联邦学习框架下，模型的训练效率往往是非常低的。这就是联邦学习中的通信异构问题。为了解决这一问题，我们提出了一种新的联邦学习框架（MedFed），该框架可以在不共享数据的前提下实现大规模模型的高效训练。

常见的联邦学习系统通常都是以下三种架构：1）服务端-客户端架构；2）点对点架构；3）序列式架构；
其中后两者在当下模型数据规模日趋庞大的情况下，都无法突破模型训练中的内存墙。
因为这两种方式，都要求每个模型存储完整的模型，并在本地进行模型训练，这就意味着，每个模型都需要存储完整的模型参数和中间的激活值
具体来说，随着模型大小的增加，存放单个模型所需要的空间也越大。
而每个医院（客户端）的存储空间是有限的。
简单地说，想要训练一个模型，我们需要存储模型的参数和中间的激活值。在机器数量固定的情况下，从极端的情况来说，中间的激活值
可以同过反向过程中的重计算\supercite{DBLP:journals/corr/abs-1911-13214}来减少存储空间，但是模型的参数是更难被减少的，这是因为，在机器数量不增加的情况下，当下的减少模型
存储空间的办法主要有offload\supercite{8820980, DBLP:journals/corr/abs-2101-06840, DBLP:journals/corr/abs-2104-07857}，但是受限于pcie的带宽限制，这种方法往往以牺牲训练效率为代价。
下面，我们简单计算一下存储一个模型需要的空间大小。通常每一个模型参数对应着三个部分的存储，分别是模型权重，模型梯度和优化器状态。我们用$P_{\text{model}}$表示模型参数的大小，$P_{\text{grad}}$表示模型梯度的大小，$P_{\text{opt}}$表示优化器状态的大小。那么，存储一个模型需要的空间大小为$P_{\text{model}} + P_{\text{grad}} + P_{\text{opt}}$。
下面，我们将分别计算模型在float32训练和float16训练下（bfloat16同），存储一个模型需要的空间大小。
设模型参数量为$N$，float32下一个参数占用4个字节，梯度占用4个字节，优化器状态占用8个字节（ADAM优化器需要存储动量和方差），因此，float32下存储一个模型需要的空间大小为$N \times 4 + N \times 4 + N \times 8 = 16N$。
而在float16下，一个参数占用2个字节，梯度占用2个字节，优化器状态则需要占用16个字节（包括高精度的参数、梯度的副本和动量、方差），因此，float16下存储一个模型需要的空间大小为$N \times 2 + N \times 2 + N \times 16 = 20N$。
假设我们要训练一个100B的分割模型，那么在float32下，存储一个模型需要的空间大小为1600G，而在float16下，存储一个模型需要的空间大小为2000G。
这是绝大多数医院都无法承受的存储空间。

因此，我们需要一种新的联邦学习框架，这个联邦学习框架需要满足以下几个条件：
1）拥有良好的扩展性，能够适应不同大小的模型；
2）尽可能消除非独立同分布数据和长尾数据对模型训练的影响；
3）尽可能消除不同医院之间的计算异构和通信异构，实现模型的高效训练，满足当下模型快速迭代的需求；
4）尽可能保护患者隐私，保证数据的安全性。


\section{联邦学习框架}
\ensection{Federated Learning Framework}

如图3.1所示，我们提出了一种新的联邦学习框架（MedFed），该框架主要包括4个模块，分别是：初始化模块、特征交换模块、模型训练更新模块和弹性计算模块。

我们首先着重介绍一下模型训练更新模块。模型训练更新模块是联邦学习框架的核心模块，和以往的联邦学习架构不同的是，本文所提出的MedFed框架旨在保护患者隐私的情况下，
尽可能实现模型的高效训练，而要解决模型的高效训练，首先要解决的就是计算异构和通信异构的问题。计算异构指的是不同客户端的计算能力和存储能力是不同的，
通信异构指的是不同客户端的通信带宽和延迟是不同的。前者导致了不同联邦学习客户端的模型训练是存在着巨大的差距的，后者则会导致计算资源的巨大浪费，因为为了保证计算的
正确性，模型在更新的同时是不能继续训练的。
根据上文提出的四个条件，我们将逐一介绍我们是如何在MedFed框架下解决这些问题的。


\subsection{两阶段模型分割训练策略}
\ensubsection{Model Scalability Solution}

在服务端-客户端的联邦学习框架下，如何设计模型在服务端和客户端上的分布，和客户端与服务端需要实现的功能，是框架设计的
的核心要素，本节旨在阐述模型的分割方法，与模型的参数更新的策略。
\subsection{模型分割方法}
\ensubsection{Model Splitting Method}
正如前文所提到的，现有的联邦学习框架，即使采用服务端-客户端的实现架构，也仅仅只是把参数的更新这一步放到服务端进行。
而没有考虑每一个客户端的实际存储实际上是非常受限的，这个和数据中心动则数千卡的存储空间是不可同日而语的。
在训练一个大规模模型的时候，我们需要存储模型的参数、梯度和优化器状态，如果要在每一个客户端都完整地存储以上
内容，这无疑将绝大多数的医院排除在外，也不符合人工智能应当普惠的初衷。
为了缩减每一个客户端的存储压力，在我们的medfed框架下，我们将模型切分成两个部分，一部分模型较小，存储在每一个医院客户端，
一部分模型较大，存储在服务端。也就是说，我们将模型的大部分参数存储在数据中心，而将模型的一小部分参数存储在每一个医院客户端。这种分布式的存储
方式类似于流水线并行\supercite{DBLP:journals/corr/abs-2104-04473, DBLP:journals/corr/abs-2006-09503}，这种方式的灵活性在于，我们可以轻易
地调整分配到医院客户端的模型参数的大小，以适应不同大小的模型。thapad等人提出的的splitfed\supercite{thapa2022splitfed}也采用了类似的架构，我们的联邦学习框架
也借鉴了他们的设计，但是splitfed的局限性在于：
1）splitfed存在隐私泄露的问题，因为splitfed会在数据中心存储完整的标签，这在一定程度上会泄露患者的隐私；
2）该方法只是在单机上验证了可行性，在实际的生产环境中，该方法的性能是无法保证的。
3）splitfed完全没有考虑通信异构和计算异构的问题，这就意味着，splitfed的训练效率总是受限于医院客户端中最慢的那一个，这在实际的生产环境中会导致训练效率的极大下降，对于宝贵的计算资源是一种极大的浪费。
4）splitfed虽然讲模型分割成了两个部分，但是对于客户端上存储的模型，仍然需要存储完整的优化器状态，正如前文所提到的，优化器状态往往是要比模型参数更大的，这进一步增加了客户端的存储压力。

接下来，我们将介绍我们是如何解决这些问题的。
首先，我们将模型切割成了两个部分，但是这种切割方式并不是简单的像splitfed那样将模型的前几层存储在客户端，将模型的后几层存储在服务端。
这种切割方式带来的坏处是，我们需要将每一个样本的标签仍然存储在服务端，这在一定程度上会泄露患者的隐私。
如图3.2所示

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/model_patition_method.png}
    \label{fig:diagram}
    \captionwithsource{MedFed联邦学习框架模型分割图}{}
\end{figure}


\subsection{模型参数更新策略}
\ensubsection{Model Parameter Update Strategy}



\subsection{动态批次组建}
\ensubsection{Dynamic Batching}
为了提升模型的训练速度，和减少数据中心模型的闲置时间，我们引入了动态批次组建的方法。具体来说，我们引入两个参数，全局批次大小$GBS$和局部批次大小$MBS$。
不同的医院客户端的计算能力、与数据中心的通信带宽和延迟都是不同的。因此，不同的医院客户端在给数据中心发送激活值的时候，发送的量和频率都是不同的。
在数据中心收到$MBS$个激活值之后，我们就会将这$MBS$个激活值组成一个批次，然后进行一次前向和反向传播。这一步需要的时间与数据中心分配给这个任务的算力
成正比，设这个时间为$t_{\text{calc}}$。同时，我们设置数据中心收到$MBS$个激活值的时间为$t_{\text{recv}}$。

我们进一步考虑了数据中心在处理这些批次时的效率问题。我们定义了一个效率比率$\eta$，
它是基于计算时间$t_{\text{calc}}$和接收时间$t_{\text{recv}}$的函数，用来衡量数据中心处理批次的效率。公式如下：
\begin{equation}
    \eta = \frac{t_{\text{calc}}}{t_{\text{recv}}}
\end{equation}

理想情况下，我们希望$\eta$接近于1，这意味着数据中心的计算能力与接收激活值的速度保持平衡。
这是因为，一个批次的处理时间由接收激活值的时间和前向反向传播中较长的一个决定。
当我们通过调节数据中心的算力，使得$\eta$接近于1时，我们的训练过程就如图3.2所示，可以看到，数据中心的计算能力和接收激活值的速度是保持平衡的。

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/medfed_efficiency.png}
    \label{fig:diagram}
    \captionwithsource{MedFed联邦学习框架效率图}{}
\end{figure}


在这种情况下，我们可以计算的数据中心对于算力的浪费，
因为数据中心的优化器更新不需要通信，因此在耗时上总是更短的，设数据中心的优化器更新时间为$t_{\text{opt\_center}}$，医院客户端的优化器更新时间为$t_{\text{opt\_client}}$。
则数据中心对的有效率计算时间为：
\begin{equation}
    t_\text{efficiency} = \frac{\frac{GBS}{MBS} * t_{\text{calc}} + t_\text{opt\_center}}{\frac{GBS}{MBS} * t_{\text{calc}} + 1 * t_{\text{calc}} + t_{\text{opt\_client}}}
\end{equation}



\subsection{数据异构解决方案}
\ensubsection{Data Heterogeneity Solution}
第二个问题是尽可能消除非独立同分布数据对模型训练的影响。数据的异构性一直是联邦学习中的一个重要问题\supercite{DBLP:journals/corr/abs-2106-06843}，因为不同的医院客户端的数据分布是不同的，如果不做任何处理，
这就会导致模型的训练效果很差。首先我们确保来自不同的数据分布的数据在一次训练中是均匀分布的（需要引用）。。。。。

\subsection{历史特征交换采样}
\ensubsection{Stable Feature Fusion}
其次根据我们所采用的实际的分割模型（存在一个教师模型，和一个学生模型，学生模型通过指数平均算法更新教师模型\supercite{DBLP:journals/corr/TarvainenV17}）
也就是教师模型是更新的相对缓慢的，因此对同一个样本而言，输入到相邻训练步数的教师模型中，得到的特征是相似的，因此我们可以定义一个历史窗口，
这个历史窗口内的模型我们认为他们是有效的模型，但是距离当前时间点越远的模型，他的权重会越小，公式如下：
\begin{equation}
    \omega_{t} = \frac{1}{1 + \alpha(t_{\text{cur} - t})}
\end{equation}
其中$\omega_{t}$表示模型的权重，$t_{\text{cur}}$表示当前时间点，$t$表示模型的时间点，$\alpha$表示一个超参数，用来调整模型的权重。



\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/medfed_framework.png}
    \label{fig:diagram}
    \captionwithsource{MedFed联邦学习框架结构图}{}
\end{figure}

\section{实验设置}
\ensection{Experimental Settings}





